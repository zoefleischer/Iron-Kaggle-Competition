{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- IMPORTING LIBRARIES------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.read_csv('sales.csv')\n",
    "sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- CLEANING THE DATA--------------------------\n",
    "\n",
    "# dropping \"Unnamed\" for too many unique values and questionable purpose, \"date\" and \"open\" for lack of purpose for the algorythm (the \"weekday\" column information suffices)\n",
    "sales_data.info\n",
    "sales_data['Unnamed: 0'].unique()\n",
    "sales_data.drop(columns=['Unnamed: 0', 'date', 'open'], inplace=True)\n",
    "\n",
    "\n",
    "#checking if there is a difference in sales between the different classes of holidays i.e. if we should just make two groups (holiday vs. non-holiday) or keep 3 different classes of holidays\n",
    "sales_data.groupby(by='state_holiday').agg({'sales':'sum'})\n",
    "\n",
    "# replacing the letters in \"state holiday\" with numbers \n",
    "sales_data['state_holiday'].unique()\n",
    "sales_data['state_holiday'] = np.where(sales_data['state_holiday'] == 'a', 1 , sales_data['state_holiday'])\n",
    "sales_data['state_holiday'] = np.where(sales_data['state_holiday'] == 'b', 2 , sales_data['state_holiday'])\n",
    "sales_data['state_holiday'] = np.where(sales_data['state_holiday'] == 'c', 3 , sales_data['state_holiday'])\n",
    "sales_data['state_holiday'] = np.where(sales_data['state_holiday'] == '0', 0 , sales_data['state_holiday'])\n",
    "sales_data['state_holiday'] = sales_data['state_holiday'].astype(int)\n",
    "\n",
    "# checking if any additional columns need to be dropped for high correlation\n",
    "sales_data.corr()\n",
    "\n",
    "# checking if standardization of data is necessary\n",
    "ax = sns.boxplot(y=\"nb_customers_on_day\", data=sales_data, palette=\"Set3\")\n",
    "# it is necessary since we have lots of outliers in a very decisive column\n",
    "\n",
    "#standardizing all columns but the target column\n",
    "sales_data_standardize = pd.DataFrame(preprocessing.StandardScaler().fit_transform(sales_data.drop(columns='sales')), columns=['store_ID', 'day_of_week', 'nb_customers_on_day', 'promotion','state_holiday', 'school_holiday'])\n",
    "sales_data_standardize['sales']=sales_data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------TRAIN TEST SPLIT--------------\n",
    "features = list(sales_data_standardize.columns)\n",
    "features.remove('sales')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = sales_data_standardize['sales']\n",
    "X = sales_data_standardize[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------TRYING OUT VARIOUS ENSEMBLE REGRESSORS-------------\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=1)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagging_reg = BaggingRegressor(\n",
    "    DecisionTreeRegressor(max_depth=3), \n",
    "    n_estimators=10, \n",
    "    max_samples=100,\n",
    "    random_state=1) \n",
    "\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "bagging_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=10, \n",
    "                               max_depth=3, \n",
    "                               random_state=1)\n",
    "forest.fit(X_train, y_train)\n",
    "forest.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), \n",
    "                            n_estimators=10,\n",
    "                            random_state=1 \n",
    "                            )\n",
    "ada_reg.fit(X_train, y_train)\n",
    "ada_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_reg = GradientBoostingRegressor(max_depth=5, \n",
    "                                   n_estimators=100,\n",
    "                                   random_state=1\n",
    "                                   )\n",
    "gb_reg.fit(X_train, y_train)\n",
    "gb_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgb_reg = xgboost.XGBRegressor(max_depth=13, n_estimators=550, n_jobs=10, random_state=0)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "xgb_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------HYPERPARAMETER TUNING------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Checking the accuracy for different max_depth\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "max_depth = range(10, 20)\n",
    "for depth in max_depth:\n",
    "    xgb_reg = xgboost.XGBRegressor(max_depth=depth, n_estimators = 550 ,random_state=0)\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "    training_accuracy.append(xgb_reg.score(X_train, y_train))\n",
    "    test_accuracy.append(xgb_reg.score(X_test, y_test))\n",
    "\n",
    "plt.plot(max_depth, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(max_depth, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# First Grid Search\n",
    "n_estimators = [10,100,500,1000]\n",
    "max_depth = [5,10]\n",
    "\n",
    "grid = {'n_estimators': n_estimators,\n",
    "        'n_jobs':10,\n",
    "        'max_depth': max_depth}\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator = xgb_reg, param_grid = grid, cv = 5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Second Grid Search (Adjusted Parameters)\n",
    "n_estimators = [400,450,500,550,600]\n",
    "max_depth = [10,11,12,13,14,15,16]\n",
    "\n",
    "grid = {'n_estimators': n_estimators,\n",
    "        'n_jobs':10,\n",
    "        'max_depth': max_depth}\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "grid_search = GridSearchCV(estimator = xgb_reg, param_grid = grid, cv = 5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.score(X_test, y_test))\n",
    "\n",
    "# Checking accuracy for narrowed down n_estimators\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "n_estimators = range(520,540)\n",
    "for est in n_estimators:\n",
    "    xgb_reg = xgboost.XGBRegressor(n_estimators=est)\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "    training_accuracy.append(xgb_reg.score(X_train, y_train))\n",
    "    test_accuracy.append(xgb_reg.score(X_test, y_test))\n",
    "\n",
    "plt.plot(n_estimators, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(n_estimators, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------SAVING THE BEST MODEL TO A PICKLE-----------\n",
    "import pickle\n",
    "pickle.dump(xgb_reg, open('model2.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
